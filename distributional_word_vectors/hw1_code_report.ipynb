{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTIC 31190 - HW1\n",
    "#### Yingzi Jin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as iter\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributional_counting(text_file, V, Vc, w):\n",
    "    \"\"\"\n",
    "    Computes a distributional counting of word co-occurrences within a given window size.\n",
    "\n",
    "    Input:\n",
    "        test_file (string): text file\n",
    "        V (set): V \n",
    "        Vc (set): Vc \n",
    "        w (int): window size\n",
    "    \n",
    "    Returns:\n",
    "        counts: dict\n",
    "    \"\"\"\n",
    "    \n",
    "    counts = {}\n",
    "\n",
    "    with open(text_file, 'r') as f:\n",
    "        for line in tqdm.tqdm(f.readlines()):\n",
    "            sentence = line.strip().split()\n",
    "            m = len(sentence)\n",
    "            \n",
    "            for i, x in enumerate(sentence):\n",
    "                if x in V:\n",
    "                    left = max(0, i - w) \n",
    "                    right = min(m, i + w + 1)\n",
    "                    \n",
    "                    for j in iter.chain(range(left, i), range(i + 1, right)):\n",
    "                        y = sentence[j]\n",
    "                        if y in Vc:\n",
    "                            counts[(x, y)] = counts.get((x, y), 0) + 1 \n",
    "                                 \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idf(text_file, Vc):\n",
    "    \"\"\"\n",
    "    Computes the Inverse Document Frequency (IDF) for each word in Vc.\n",
    "    \n",
    "    Inputs:\n",
    "        test_file (string): text file\n",
    "        Vc (set): Vc \n",
    "    \n",
    "    Returns:\n",
    "        idfs_dict: dict\n",
    "        S: int\n",
    "    \"\"\"\n",
    "    S = 0\n",
    "    idfs_dict = dict.fromkeys(list(Vc), 0)\n",
    "\n",
    "    with open(text_file, 'r') as f:\n",
    "        for line in tqdm.tqdm(f.readlines()):\n",
    "            sentence = line.strip().split()\n",
    "            S += 1\n",
    "            for y in set(sentence):\n",
    "                if y in Vc:  \n",
    "                    idfs_dict[y] += 1\n",
    "\n",
    "\n",
    "    return idfs_dict, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(counts, Vc):\n",
    "    \"\"\"\n",
    "    Creates word vectors based on co-occurrence counts with context words.\n",
    "    \n",
    "    Inputs:\n",
    "        counts(dict): Dictionary with keys as word pairs (x, y) and values as \n",
    "            co-occurrence counts\n",
    "        Vc (set): Vc \n",
    "    \n",
    "    Returns:\n",
    "        vectors (dict): word vectors\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    vc_dim = len(Vc)\n",
    "    vc_list = list(Vc)\n",
    "    vectors = {}\n",
    "    y_indices = {word: index for index, word in enumerate(vc_list)}\n",
    "\n",
    "    for (x, y), value in tqdm.tqdm(counts.items()):\n",
    "        if x not in vectors:\n",
    "            vectors[x] = np.zeros(vc_dim)\n",
    "\n",
    "        y_index = y_indices[y]\n",
    "        vectors[x][y_index] = value\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_vocab(filename):\n",
    "    \"\"\"\n",
    "    Imports vocabulary file.\n",
    "\n",
    "    Input: \n",
    "        filename (string): vocabulary file\n",
    "\n",
    "    Returns:\n",
    "        a set \n",
    "    \"\"\"\n",
    "   \n",
    "    return set(open(filename).read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors.\n",
    "\n",
    "    Inputs:\n",
    "    vec1 (numpy.ndarray): The first vector\n",
    "    vec2 (numpy.ndarray): The second vector\n",
    "\n",
    "    Returns:\n",
    "    float: The cosine similarity between vec1 and vec2, ranging from -1 to 1.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    magnitude1 = np.linalg.norm(vec1)\n",
    "    magnitude2 = np.linalg.norm(vec2)\n",
    "\n",
    "    return (dot_product / (magnitude1 * magnitude2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_spearman(vectors_dict, wordpairs):\n",
    "    \"\"\"\n",
    "    Calculates the spearman correlations between annoted scores and word vector scores.\n",
    "\n",
    "    Inputs:\n",
    "        vectors_dict (dict): methods as keys, the word vectors as values\n",
    "        wordpairs (string): filename\n",
    "\n",
    "    Returns:\n",
    "        spearmans (dict): methods as key, correlation coefficient as value\n",
    "    \n",
    "    \"\"\"\n",
    "    scores_dict = defaultdict(list)\n",
    "    annoted_scores = []\n",
    "    spearmans = {}\n",
    "    \n",
    "    with open(wordpairs, 'r') as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            inputs = line.strip().split()\n",
    "            word1 = inputs[0]\n",
    "            word2 = inputs[1]\n",
    "            words = [word1, word2]\n",
    "            annoted_scores.append(float(inputs[2]))\n",
    "            \n",
    "            for method, vectors in vectors_dict.items():\n",
    "                if word1 not in vectors or word2 not in vectors:\n",
    "                    cos = 0\n",
    "                else:\n",
    "                    vec1 = vectors[word1]\n",
    "                    vec2 = vectors[word2]\n",
    "                    cos = cosine_similarity(vec1, vec2)\n",
    "\n",
    "                scores_dict[method].append(cos)\n",
    "\n",
    "    annoted_scores = np.array(annoted_scores)\n",
    "    for method in vectors_dict:\n",
    "        scores = np.array(scores_dict[method])\n",
    "        cc, _ = spearmanr(scores, annoted_scores)\n",
    "        spearmans[method] = cc\n",
    "\n",
    "    return spearmans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_vector(vectors, tfidf=False, pmi=False, idf=None, S=None):\n",
    "    \"\"\"\n",
    "    Transform word vecotrs according to methods. \n",
    "\n",
    "    Inputs:\n",
    "        vectors (dict): word vectors based on counts\n",
    "        tfidf (=False): True if method is tfidf\n",
    "        pmi (=False): True is method is pmi\n",
    "        idf (dict): dictionary of IDFs for each word in Vc\n",
    "        S: int\n",
    "\n",
    "    Returns:\n",
    "        a dict of transformed word vecotrs \n",
    "    \"\"\"\n",
    "    vector_key_list  = list(vectors.keys())\n",
    "    matrix = np.vstack(list(vectors.values()))\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        if pmi:\n",
    "            px = np.sum(matrix, axis=1)\n",
    "            py = np.sum(matrix, axis=0)\n",
    "            N = np.sum(matrix)\n",
    "\n",
    "            val = np.log2(matrix * N / np.outer(px, py))\n",
    "            val[~np.isfinite(val)] = 0\n",
    "\n",
    "        if tfidf:\n",
    "            idf_array = np.array(list(idf.values()))\n",
    "            val = np.where(idf_array != 0, matrix * (S / idf_array), 0)\n",
    "        \n",
    "    \n",
    "    return dict(zip(vector_key_list, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_bottom_n_pmi(vectors_pmi, ctrword, Vc, n):\n",
    "    \"\"\"\n",
    "    Select context words that have the top n and bottem n pmi values with a center word.\n",
    "\n",
    "    Inputs:\n",
    "        pmi_vectors (dict): word vectors transformed based on pmi\n",
    "        ctrword (string): center word\n",
    "        Vc: set\n",
    "        n: int\n",
    "\n",
    "    Returns:\n",
    "        top_n_dict: dict\n",
    "        bottom_n_dict: dict\n",
    "    \"\"\"\n",
    "    \n",
    "    values = vectors_pmi[ctrword]\n",
    "\n",
    "    top_n_indices = values.argsort()[-n:][::-1]\n",
    "    bottom_n_indices = values.argsort()[:n]\n",
    "\n",
    "    vc_array = np.array(list(Vc))\n",
    "\n",
    "    top_n_dict = {}\n",
    "    bottom_n_dict = {}\n",
    "\n",
    "    for idx in top_n_indices:\n",
    "        top_n_dict[vc_array[idx]] = values[idx]\n",
    "\n",
    "    for idx in bottom_n_indices:\n",
    "        bottom_n_dict[vc_array[idx]] = values[idx]\n",
    "\n",
    "    return top_n_dict, bottom_n_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison(methods, window_sizes, text_file, V_file, Vc_files, wordpairs_files):\n",
    "    \"\"\"\n",
    "    Compares the spearman scores for different methods, Vcs, wordpairs files, and window sizes. \n",
    "\n",
    "    Inputs:\n",
    "        methods (list of strings): methods applied to the word vectors\n",
    "        window_sizes: int\n",
    "        text_file: string\n",
    "        V_file: string\n",
    "        Vc_files: list of strings\n",
    "        wordpairs_files: list of strings\n",
    "\n",
    "    Returns: \n",
    "        results: dict\n",
    "    \"\"\"\n",
    "\n",
    "    V = import_vocab(V_file)\n",
    "    results = {}\n",
    "\n",
    "    for Vc_file in Vc_files:\n",
    "        Vc = import_vocab(Vc_file)\n",
    "\n",
    "        if \"IDF\" in methods:\n",
    "            idf_dict, S = calculate_idf(text_file, Vc)\n",
    "\n",
    "        for w in window_sizes:\n",
    "            counts =  distributional_counting(text_file, V, Vc, w)\n",
    "            vectors = word_vector(counts, Vc)\n",
    "            vectors_dict = {}\n",
    "            \n",
    "            if \"counts\" in methods:\n",
    "                vectors_dict['counts'] = vectors\n",
    "\n",
    "            if \"IDF\" in methods:\n",
    "                vectors_tfidf = transform_vector(vectors, tfidf=True, idf=idf_dict, S=S)\n",
    "                vectors_dict['IDF'] = vectors_tfidf\n",
    "\n",
    "            if \"PMI\" in methods:\n",
    "                vectors_pmi = transform_vector(vectors, pmi=True)\n",
    "                vectors_dict['PMI'] = vectors_pmi\n",
    "\n",
    "            for wordpairs_file in wordpairs_files:\n",
    "                spearmans = calculate_spearman(vectors_dict, wordpairs_file)\n",
    "                results[(Vc_file.split(\"/\")[-1], wordpairs_file.split(\"/\")[-1], w)] = spearmans\n",
    "\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_cosine(query, vectors_pmi_w):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between a query and the rest with pmi.\n",
    "\n",
    "    Inputs:\n",
    "        query: string\n",
    "        vectors_pmi_w: dict of dicts, window sizes as keys, and corresponded vectors_pmi as values\n",
    "    \n",
    "    Returns:\n",
    "        scores_dict: dict of dicts\n",
    "    \"\"\"\n",
    "    scores_dict = defaultdict(dict)\n",
    "    for w, vectors_pmi in vectors_pmi_w.items():\n",
    "         query_vector = vectors_pmi[query]\n",
    "         for word, vector in tqdm.tqdm(vectors_pmi.items()):\n",
    "            if word != query:\n",
    "                cos = cosine_similarity(query_vector, vector)\n",
    "                scores_dict[w][word] = cos\n",
    "    \n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_nearest(query_scores, n, w):\n",
    "    \"\"\"\n",
    "    Find the n nearest neighbors of a query with a window size. \n",
    "    \"\"\"\n",
    "    d = query_scores[w]\n",
    "    return [k for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queries_n_nearest(query_category, vectors_pmi_w, n):\n",
    "    \"\"\"\n",
    "    Find the n nearest neighbors with multiple queries. \n",
    "    \"\"\"\n",
    "    queries_nearest = {}\n",
    "    for category, queries in query_category.items():\n",
    "        for query in queries:\n",
    "            query_scores = query_cosine(query, vectors_pmi_w)\n",
    "            for w in query_scores:\n",
    "                nearest_n = n_nearest(query_scores, n, w)\n",
    "                queries_nearest[(w, category, query)] = nearest_n\n",
    "    \n",
    "    df = pd.DataFrame(queries_nearest)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = \"hw1-data/wiki-1percent.txt\"\n",
    "V_sample = [\"chicken\", \"chicago\", \"coffee\"]\n",
    "Vc_sample = [\"the\", \"wings\", \"chicago\", \"cup\", \"coffee\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997898/997898 [00:03<00:00, 281097.48it/s]\n"
     ]
    }
   ],
   "source": [
    "counts_sample_3 = distributional_counting(text_file, V_sample, Vc_sample, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('chicago', 'the'): 875, ('coffee', 'the'): 95, ('chicago', 'chicago'): 38, ('chicken', 'the'): 52, ('coffee', 'cup'): 10, ('chicken', 'wings'): 6, ('coffee', 'coffee'): 4, ('chicago', 'cup'): 1}\n"
     ]
    }
   ],
   "source": [
    "print(counts_sample_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997898/997898 [00:03<00:00, 294473.08it/s]\n"
     ]
    }
   ],
   "source": [
    "counts_sample_6 = distributional_counting(text_file, V_sample, Vc_sample, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('chicago', 'the'): 1467, ('coffee', 'the'): 201, ('chicago', 'chicago'): 122, ('chicken', 'the'): 103, ('coffee', 'cup'): 14, ('coffee', 'coffee'): 36, ('chicken', 'wings'): 7, ('chicago', 'cup'): 7, ('chicago', 'wings'): 2}\n"
     ]
    }
   ],
   "source": [
    "print(counts_sample_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_file = \"hw1-data/vocab-15kws.txt\"\n",
    "Vc_file = \"hw1-data/vocab-5k.txt\"\n",
    "wordpairs_files = [\"hw1-data/simlex-999.txt\", \"hw1-data/men.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = import_vocab(V_file)\n",
    "Vc = import_vocab(Vc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997898/997898 [01:40<00:00, 9970.88it/s] \n"
     ]
    }
   ],
   "source": [
    "counts = distributional_counting(text_file, V, Vc, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7692505/7692505 [00:07<00:00, 1093193.62it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors = word_vector(counts, Vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_dict = {'counts': vectors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpairs_cc = {}\n",
    "for wordpairs in wordpairs_files:\n",
    "    cc = calculate_spearman(vectors_dict, wordpairs)\n",
    "    wordpairs_cc[wordpairs.split(\"/\")[-1]] = cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'simlex-999.txt': {'counts': 0.0587613533134978},\n",
       " 'men.txt': {'counts': 0.2251396048448754}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpairs_cc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997898/997898 [00:07<00:00, 137263.47it/s]\n"
     ]
    }
   ],
   "source": [
    "idf, S = calculate_idf(text_file, Vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_idf = transform_vector(vectors, tfidf=True, idf=idf, S=S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_dict = {\"IDF\": vectors_idf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpairs_cc = {}\n",
    "for wordpairs in wordpairs_files:\n",
    "    cc = calculate_spearman(vectors_dict, wordpairs)\n",
    "    wordpairs_cc[wordpairs.split(\"/\")[-1]] = cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'simlex-999.txt': {'IDF': 0.1643113945921928},\n",
       " 'men.txt': {'IDF': 0.47281906258988254}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpairs_cc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_pmi = transform_vector(vectors, pmi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrword = \"coffee\"\n",
    "top, bottem = top_bottom_n_pmi(vectors_pmi, ctrword, Vc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tea': 8.16600126243293,\n",
       " 'drinking': 7.58797865873193,\n",
       " 'shop': 7.411693771493207,\n",
       " 'costa': 7.350256393786161,\n",
       " 'shops': 7.260751873418467,\n",
       " 'sugar': 6.533949521544205,\n",
       " 'coffee': 6.501977131805925,\n",
       " 'mix': 6.131195903101976,\n",
       " 'seattle': 5.950816325067398,\n",
       " 'houses': 5.868161497268183}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'he': -2.26033826495274,\n",
       " 'be': -2.1509730526875237,\n",
       " 'had': -1.9875291676196303,\n",
       " 'this': -1.979549817934235,\n",
       " 'not': -1.9115928402014317,\n",
       " 'its': -1.839457915441101,\n",
       " 'after': -1.598505205571959,\n",
       " 'more': -1.4785257922880328,\n",
       " 'when': -1.4043486976803334,\n",
       " 'page': -1.2805627423998573}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_dict = {\"PMI\": vectors_pmi}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpairs_cc = {}\n",
    "for wordpairs in wordpairs_files:\n",
    "    cc = calculate_spearman(vectors_dict, wordpairs)\n",
    "    wordpairs_cc[wordpairs.split(\"/\")[-1]] = cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'simlex-999.txt': {'PMI': 0.18643183126956037},\n",
       " 'men.txt': {'PMI': 0.4656324083603801}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpairs_cc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['counts', 'IDF', 'PMI']\n",
    "window_sizes = [1, 3, 6]\n",
    "Vc_files = [\"hw1-data/vocab-5k.txt\", \"hw1-data/vocab-15kws.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997898/997898 [00:08<00:00, 122775.82it/s]\n",
      "100%|██████████| 997898/997898 [01:09<00:00, 14293.04it/s]\n",
      "100%|██████████| 2757620/2757620 [00:04<00:00, 599674.49it/s]\n",
      "100%|██████████| 997898/997898 [01:56<00:00, 8557.05it/s] \n",
      "100%|██████████| 7692505/7692505 [00:09<00:00, 831192.89it/s] \n",
      "100%|██████████| 997898/997898 [03:39<00:00, 4540.07it/s]\n",
      "100%|██████████| 12186839/12186839 [00:18<00:00, 644901.84it/s]\n",
      "100%|██████████| 997898/997898 [00:11<00:00, 90200.80it/s] \n",
      "100%|██████████| 997898/997898 [01:07<00:00, 14702.58it/s]\n",
      "100%|██████████| 3693994/3693994 [00:11<00:00, 309433.45it/s]\n",
      "100%|██████████| 997898/997898 [02:20<00:00, 7095.16it/s] \n",
      "100%|██████████| 10549201/10549201 [00:38<00:00, 272654.61it/s]\n",
      "100%|██████████| 997898/997898 [03:59<00:00, 4167.92it/s] \n",
      "100%|██████████| 17199804/17199804 [01:28<00:00, 193975.98it/s]\n"
     ]
    }
   ],
   "source": [
    "results = comparison(methods, window_sizes, text_file, V_file, Vc_files, wordpairs_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "      <th>IDF</th>\n",
       "      <th>PMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">vocab-5k.txt</th>\n",
       "      <th>simlex-999.txt</th>\n",
       "      <th>1</th>\n",
       "      <td>0.067786</td>\n",
       "      <td>0.189229</td>\n",
       "      <td>0.227498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>men.txt</th>\n",
       "      <th>1</th>\n",
       "      <td>0.209092</td>\n",
       "      <td>0.347559</td>\n",
       "      <td>0.433603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simlex-999.txt</th>\n",
       "      <th>3</th>\n",
       "      <td>0.058761</td>\n",
       "      <td>0.164311</td>\n",
       "      <td>0.186432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>men.txt</th>\n",
       "      <th>3</th>\n",
       "      <td>0.225140</td>\n",
       "      <td>0.472819</td>\n",
       "      <td>0.465632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simlex-999.txt</th>\n",
       "      <th>6</th>\n",
       "      <td>0.044696</td>\n",
       "      <td>0.110603</td>\n",
       "      <td>0.150331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>men.txt</th>\n",
       "      <th>6</th>\n",
       "      <td>0.241067</td>\n",
       "      <td>0.532399</td>\n",
       "      <td>0.472408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">vocab-15kws.txt</th>\n",
       "      <th>simlex-999.txt</th>\n",
       "      <th>1</th>\n",
       "      <td>0.070014</td>\n",
       "      <td>0.187200</td>\n",
       "      <td>0.268065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>men.txt</th>\n",
       "      <th>1</th>\n",
       "      <td>0.206398</td>\n",
       "      <td>0.366168</td>\n",
       "      <td>0.470237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simlex-999.txt</th>\n",
       "      <th>3</th>\n",
       "      <td>0.057142</td>\n",
       "      <td>0.147853</td>\n",
       "      <td>0.212292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>men.txt</th>\n",
       "      <th>3</th>\n",
       "      <td>0.220778</td>\n",
       "      <td>0.480952</td>\n",
       "      <td>0.519393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simlex-999.txt</th>\n",
       "      <th>6</th>\n",
       "      <td>0.040654</td>\n",
       "      <td>0.108782</td>\n",
       "      <td>0.160912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>men.txt</th>\n",
       "      <th>6</th>\n",
       "      <td>0.236911</td>\n",
       "      <td>0.525109</td>\n",
       "      <td>0.527416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    counts       IDF       PMI\n",
       "vocab-5k.txt    simlex-999.txt 1  0.067786  0.189229  0.227498\n",
       "                men.txt        1  0.209092  0.347559  0.433603\n",
       "                simlex-999.txt 3  0.058761  0.164311  0.186432\n",
       "                men.txt        3  0.225140  0.472819  0.465632\n",
       "                simlex-999.txt 6  0.044696  0.110603  0.150331\n",
       "                men.txt        6  0.241067  0.532399  0.472408\n",
       "vocab-15kws.txt simlex-999.txt 1  0.070014  0.187200  0.268065\n",
       "                men.txt        1  0.206398  0.366168  0.470237\n",
       "                simlex-999.txt 3  0.057142  0.147853  0.212292\n",
       "                men.txt        3  0.220778  0.480952  0.519393\n",
       "                simlex-999.txt 6  0.040654  0.108782  0.160912\n",
       "                men.txt        6  0.236911  0.525109  0.527416"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1\n",
    "\n",
    "In general, PMI outperforms IDF and Counts (Counts always perfoms the worst), with some exceptions from `MEN.txt` using `vocab-5k.txt` and window size 3 and 6, where IDF performs the best. \n",
    "\n",
    "PMI's ability to capture the strength of word associations based on co-occurrence patterns makes it generally more effective for tasks related to word semantics, such as word similarity measurements in our case. \n",
    "\n",
    "Also, across all context vocabularies, window sizes, all three methods performs much better on `Men` than `Simlex`.\n",
    "\n",
    "When context vocabulary changes, the correlations generally are very similar between the two vocabularies. The differences are minor, suggesting that the context vocabulary doesn't dramatically change the quality of the word vectors in this dataset.\n",
    "\n",
    "While correlations for all methods decrease with larger windows for `SimLex`, they increase for `Men`.\n",
    "\n",
    "\n",
    "#### 4.2 \n",
    "\n",
    "For `Men`, the similarity scores are determined more based on relatedness (themes) than senses (semantics). For example, \"morning\" and \"sunrise\" are assigned very high similarity score in `Men`, but they are not synonyms. Therefore, a larger window which allows capturing more meaningful semantic relationships lead to higher correlations with `Men`. \n",
    "\n",
    "For `Simlex`, unlike `Men`, the similarity scores are determined more based on senses. For example, \"reader\" and \"author\" are annotated as very low similarity in `Simlex` since they are not synonyms, but they are closely related in terms of theme. Larger windows that can include more noise and less specific context tend to capture relatedness more than pure similarity in senses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vc = import_vocab(\"hw1-data/vocab-5k.txt\")\n",
    "window_sizes = [1, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997898/997898 [01:03<00:00, 15740.90it/s]\n",
      "100%|██████████| 2757620/2757620 [00:03<00:00, 837369.72it/s] \n",
      "100%|██████████| 997898/997898 [03:10<00:00, 5231.97it/s]\n",
      "100%|██████████| 12186839/12186839 [00:18<00:00, 645420.99it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors_pmi_w = {}\n",
    "for w in window_sizes:\n",
    "    counts = distributional_counting(text_file, V, Vc, w)\n",
    "    vectors = word_vector(counts, Vc)\n",
    "    vectors_pmi = transform_vector(vectors, pmi=True)\n",
    "    vectors_pmi_w[w] = vectors_pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"judges\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15225/15225 [00:01<00:00, 12508.88it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 52084.18it/s]\n"
     ]
    }
   ],
   "source": [
    "query_scores = query_cosine(query, vectors_pmi_w)\n",
    "nearest_10_1 = n_nearest(query_scores, 10, 1)\n",
    "nearest_10_6 = n_nearest(query_scores, 10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['judge',\n",
       " 'players',\n",
       " 'appeals',\n",
       " 'officials',\n",
       " 'ministers',\n",
       " 'justices',\n",
       " 'leaders',\n",
       " 'members',\n",
       " 'unanimously',\n",
       " 'contestants']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_10_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['judge',\n",
       " 'jury',\n",
       " 'appeals',\n",
       " 'courts',\n",
       " 'panel',\n",
       " 'supreme',\n",
       " 'justice',\n",
       " 'contestants',\n",
       " 'candidates',\n",
       " 'appeal']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_10_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_category = {\"nouns\": [\"dog\", \"goal\", \"table\", \"vehicle\"],\n",
    "                  \"verbs\": [\"pass\", \"passing\", \"passed\"],\n",
    "                  \"adjs\": [\"happy\", \"negative\", \"visible\"],\n",
    "                  \"preps\": [\"from\", \"above\", \"after\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15225/15225 [00:01<00:00, 12486.26it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 44359.30it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 27243.87it/s]\n",
      "100%|██████████| 15225/15225 [00:01<00:00, 12275.79it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 60265.49it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 59348.34it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 61837.97it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 65732.98it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 71177.14it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 71009.90it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 68298.72it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 71130.92it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 70230.58it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 70926.45it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 71094.01it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 67776.65it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 70905.66it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 71091.40it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 71369.97it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 54872.57it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 68300.18it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 68698.94it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 70007.45it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 66265.37it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 67064.57it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 66947.15it/s]\n"
     ]
    }
   ],
   "source": [
    "df = queries_n_nearest(query_category, vectors_pmi_w, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>nouns</th>\n",
       "      <th>nouns</th>\n",
       "      <th>nouns</th>\n",
       "      <th>nouns</th>\n",
       "      <th>nouns</th>\n",
       "      <th>nouns</th>\n",
       "      <th>nouns</th>\n",
       "      <th>nouns</th>\n",
       "      <th>verbs</th>\n",
       "      <th>verbs</th>\n",
       "      <th>...</th>\n",
       "      <th>adjs</th>\n",
       "      <th>adjs</th>\n",
       "      <th>adjs</th>\n",
       "      <th>adjs</th>\n",
       "      <th>preps</th>\n",
       "      <th>preps</th>\n",
       "      <th>preps</th>\n",
       "      <th>preps</th>\n",
       "      <th>preps</th>\n",
       "      <th>preps</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>dog</th>\n",
       "      <th>dog</th>\n",
       "      <th>goal</th>\n",
       "      <th>goal</th>\n",
       "      <th>table</th>\n",
       "      <th>table</th>\n",
       "      <th>vehicle</th>\n",
       "      <th>vehicle</th>\n",
       "      <th>pass</th>\n",
       "      <th>pass</th>\n",
       "      <th>...</th>\n",
       "      <th>negative</th>\n",
       "      <th>negative</th>\n",
       "      <th>visible</th>\n",
       "      <th>visible</th>\n",
       "      <th>from</th>\n",
       "      <th>from</th>\n",
       "      <th>above</th>\n",
       "      <th>above</th>\n",
       "      <th>after</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>girl</td>\n",
       "      <td>purpose</td>\n",
       "      <td>goals</td>\n",
       "      <td>category</td>\n",
       "      <td>lists</td>\n",
       "      <td>car</td>\n",
       "      <td>vehicles</td>\n",
       "      <td>passes</td>\n",
       "      <td>passes</td>\n",
       "      <td>...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>noticeable</td>\n",
       "      <td>surface</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>below</td>\n",
       "      <td>page</td>\n",
       "      <td>before</td>\n",
       "      <td>before</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>turtle</td>\n",
       "      <td>cat</td>\n",
       "      <td>goals</td>\n",
       "      <td>scored</td>\n",
       "      <td>map</td>\n",
       "      <td>tables</td>\n",
       "      <td>vehicles</td>\n",
       "      <td>engine</td>\n",
       "      <td>run</td>\n",
       "      <td>drive</td>\n",
       "      <td>...</td>\n",
       "      <td>critical</td>\n",
       "      <td>reaction</td>\n",
       "      <td>evident</td>\n",
       "      <td>shape</td>\n",
       "      <td>between</td>\n",
       "      <td>between</td>\n",
       "      <td>here</td>\n",
       "      <td>should</td>\n",
       "      <td>when</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bear</td>\n",
       "      <td>boy</td>\n",
       "      <td>aim</td>\n",
       "      <td>scoring</td>\n",
       "      <td>tables</td>\n",
       "      <td>template</td>\n",
       "      <td>aircraft</td>\n",
       "      <td>fuel</td>\n",
       "      <td>passed</td>\n",
       "      <td>touchdown</td>\n",
       "      <td>...</td>\n",
       "      <td>adverse</td>\n",
       "      <td>critical</td>\n",
       "      <td>accessible</td>\n",
       "      <td>relatively</td>\n",
       "      <td>through</td>\n",
       "      <td>,</td>\n",
       "      <td>debate</td>\n",
       "      <td>discussion</td>\n",
       "      <td>while</td>\n",
       "      <td>during</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>goat</td>\n",
       "      <td>horse</td>\n",
       "      <td>match</td>\n",
       "      <td>score</td>\n",
       "      <td>section</td>\n",
       "      <td>bottom</td>\n",
       "      <td>boat</td>\n",
       "      <td>speed</td>\n",
       "      <td>reach</td>\n",
       "      <td>passing</td>\n",
       "      <td>...</td>\n",
       "      <td>favorable</td>\n",
       "      <td>effects</td>\n",
       "      <td>recognizable</td>\n",
       "      <td>inside</td>\n",
       "      <td>until</td>\n",
       "      <td>until</td>\n",
       "      <td>following</td>\n",
       "      <td>talk</td>\n",
       "      <td>during</td>\n",
       "      <td>later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>horse</td>\n",
       "      <td>animals</td>\n",
       "      <td>objectives</td>\n",
       "      <td>match</td>\n",
       "      <td>lists</td>\n",
       "      <td>text</td>\n",
       "      <td>bus</td>\n",
       "      <td>cars</td>\n",
       "      <td>drive</td>\n",
       "      <td>ball</td>\n",
       "      <td>...</td>\n",
       "      <td>significant</td>\n",
       "      <td>serious</td>\n",
       "      <td>available</td>\n",
       "      <td>color</td>\n",
       "      <td>since</td>\n",
       "      <td>at</td>\n",
       "      <td>list</td>\n",
       "      <td>below</td>\n",
       "      <td>following</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dogs</td>\n",
       "      <td>animal</td>\n",
       "      <td>position</td>\n",
       "      <td>points</td>\n",
       "      <td>file</td>\n",
       "      <td>box</td>\n",
       "      <td>ship</td>\n",
       "      <td>motor</td>\n",
       "      <td>running</td>\n",
       "      <td>yards</td>\n",
       "      <td>...</td>\n",
       "      <td>serious</td>\n",
       "      <td>behavior</td>\n",
       "      <td>noteworthy</td>\n",
       "      <td>objects</td>\n",
       "      <td>into</td>\n",
       "      <td>south</td>\n",
       "      <td>review</td>\n",
       "      <td>link</td>\n",
       "      <td>since</td>\n",
       "      <td>when</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hat</td>\n",
       "      <td>joe</td>\n",
       "      <td>concern</td>\n",
       "      <td>win</td>\n",
       "      <td>text</td>\n",
       "      <td>results</td>\n",
       "      <td>automobile</td>\n",
       "      <td>equipment</td>\n",
       "      <td>go</td>\n",
       "      <td>passed</td>\n",
       "      <td>...</td>\n",
       "      <td>strong</td>\n",
       "      <td>certain</td>\n",
       "      <td>notable</td>\n",
       "      <td>walls</td>\n",
       "      <td>by</td>\n",
       "      <td>moved</td>\n",
       "      <td>however</td>\n",
       "      <td>article</td>\n",
       "      <td>by</td>\n",
       "      <td>until</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rabbit</td>\n",
       "      <td>wild</td>\n",
       "      <td>debut</td>\n",
       "      <td>victory</td>\n",
       "      <td>template</td>\n",
       "      <td>function</td>\n",
       "      <td>traffic</td>\n",
       "      <td>car</td>\n",
       "      <td>look</td>\n",
       "      <td>goal</td>\n",
       "      <td>...</td>\n",
       "      <td>useful</td>\n",
       "      <td>impact</td>\n",
       "      <td>susceptible</td>\n",
       "      <td>yellow</td>\n",
       "      <td>,</td>\n",
       "      <td>)</td>\n",
       "      <td>talk</td>\n",
       "      <td>here</td>\n",
       "      <td>until</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>girl</td>\n",
       "      <td>steve</td>\n",
       "      <td>task</td>\n",
       "      <td>winning</td>\n",
       "      <td>below</td>\n",
       "      <td>simple</td>\n",
       "      <td>operations</td>\n",
       "      <td>traffic</td>\n",
       "      <td>passing</td>\n",
       "      <td>running</td>\n",
       "      <td>...</td>\n",
       "      <td>specific</td>\n",
       "      <td>potential</td>\n",
       "      <td>significant</td>\n",
       "      <td>structures</td>\n",
       "      <td>after</td>\n",
       "      <td>graduated</td>\n",
       "      <td>link</td>\n",
       "      <td>do</td>\n",
       "      <td>into</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pig</td>\n",
       "      <td>dogs</td>\n",
       "      <td>demise</td>\n",
       "      <td>touchdown</td>\n",
       "      <td>picture</td>\n",
       "      <td>value</td>\n",
       "      <td>engine</td>\n",
       "      <td>aircraft</td>\n",
       "      <td>turn</td>\n",
       "      <td>runs</td>\n",
       "      <td>...</td>\n",
       "      <td>personal</td>\n",
       "      <td>effect</td>\n",
       "      <td>active</td>\n",
       "      <td>skin</td>\n",
       "      <td>.</td>\n",
       "      <td>into</td>\n",
       "      <td>same</td>\n",
       "      <td>review</td>\n",
       "      <td>but</td>\n",
       "      <td>following</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1        6           1          6         1         6           1  \\\n",
       "    nouns    nouns       nouns      nouns     nouns     nouns       nouns   \n",
       "      dog      dog        goal       goal     table     table     vehicle   \n",
       "0     cat     girl     purpose      goals  category     lists         car   \n",
       "1  turtle      cat       goals     scored       map    tables    vehicles   \n",
       "2    bear      boy         aim    scoring    tables  template    aircraft   \n",
       "3    goat    horse       match      score   section    bottom        boat   \n",
       "4   horse  animals  objectives      match     lists      text         bus   \n",
       "5    dogs   animal    position     points      file       box        ship   \n",
       "6     hat      joe     concern        win      text   results  automobile   \n",
       "7  rabbit     wild       debut    victory  template  function     traffic   \n",
       "8    girl    steve        task    winning     below    simple  operations   \n",
       "9     pig     dogs      demise  touchdown   picture     value      engine   \n",
       "\n",
       "           6        1          6  ...            1          6             1  \\\n",
       "       nouns    verbs      verbs  ...         adjs       adjs          adjs   \n",
       "     vehicle     pass       pass  ...     negative   negative       visible   \n",
       "0   vehicles   passes     passes  ...     positive   positive    noticeable   \n",
       "1     engine      run      drive  ...     critical   reaction       evident   \n",
       "2       fuel   passed  touchdown  ...      adverse   critical    accessible   \n",
       "3      speed    reach    passing  ...    favorable    effects  recognizable   \n",
       "4       cars    drive       ball  ...  significant    serious     available   \n",
       "5      motor  running      yards  ...      serious   behavior    noteworthy   \n",
       "6  equipment       go     passed  ...       strong    certain       notable   \n",
       "7        car     look       goal  ...       useful     impact   susceptible   \n",
       "8    traffic  passing    running  ...     specific  potential   significant   \n",
       "9   aircraft     turn       runs  ...     personal     effect        active   \n",
       "\n",
       "            6        1          6          1           6          1          6  \n",
       "         adjs    preps      preps      preps       preps      preps      preps  \n",
       "      visible     from       from      above       above      after      after  \n",
       "0     surface       in         in      below        page     before     before  \n",
       "1       shape  between    between       here      should       when         he  \n",
       "2  relatively  through          ,     debate  discussion      while     during  \n",
       "3      inside    until      until  following        talk     during      later  \n",
       "4       color    since         at       list       below  following        his  \n",
       "5     objects     into      south     review        link      since       when  \n",
       "6       walls       by      moved    however     article         by      until  \n",
       "7      yellow        ,          )       talk        here      until        had  \n",
       "8  structures    after  graduated       link          do       into      first  \n",
       "9        skin        .       into       same      review        but  following  \n",
       "\n",
       "[10 rows x 26 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>verbs</th>\n",
       "      <th>verbs</th>\n",
       "      <th>verbs</th>\n",
       "      <th>verbs</th>\n",
       "      <th>verbs</th>\n",
       "      <th>verbs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>pass</th>\n",
       "      <th>pass</th>\n",
       "      <th>passing</th>\n",
       "      <th>passing</th>\n",
       "      <th>passed</th>\n",
       "      <th>passed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>passes</td>\n",
       "      <td>passes</td>\n",
       "      <td>crossing</td>\n",
       "      <td>passes</td>\n",
       "      <td>enacted</td>\n",
       "      <td>pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>run</td>\n",
       "      <td>drive</td>\n",
       "      <td>running</td>\n",
       "      <td>pass</td>\n",
       "      <td>adopted</td>\n",
       "      <td>act</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>passed</td>\n",
       "      <td>touchdown</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>crossing</td>\n",
       "      <td>approved</td>\n",
       "      <td>declared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reach</td>\n",
       "      <td>passing</td>\n",
       "      <td>ran</td>\n",
       "      <td>runs</td>\n",
       "      <td>changed</td>\n",
       "      <td>granted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>drive</td>\n",
       "      <td>ball</td>\n",
       "      <td>heading</td>\n",
       "      <td>drive</td>\n",
       "      <td>introduced</td>\n",
       "      <td>ran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>running</td>\n",
       "      <td>yards</td>\n",
       "      <td>runs</td>\n",
       "      <td>highway</td>\n",
       "      <td>passes</td>\n",
       "      <td>adopted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>go</td>\n",
       "      <td>passed</td>\n",
       "      <td>passes</td>\n",
       "      <td>traffic</td>\n",
       "      <td>returned</td>\n",
       "      <td>approved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>look</td>\n",
       "      <td>goal</td>\n",
       "      <td>carrying</td>\n",
       "      <td>route</td>\n",
       "      <td>dropped</td>\n",
       "      <td>legislature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>passing</td>\n",
       "      <td>running</td>\n",
       "      <td>moving</td>\n",
       "      <td>creek</td>\n",
       "      <td>voted</td>\n",
       "      <td>issued</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>turn</td>\n",
       "      <td>runs</td>\n",
       "      <td>driving</td>\n",
       "      <td>onto</td>\n",
       "      <td>carried</td>\n",
       "      <td>bill</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         1          6         1         6           1            6\n",
       "     verbs      verbs     verbs     verbs       verbs        verbs\n",
       "      pass       pass   passing   passing      passed       passed\n",
       "0   passes     passes  crossing    passes     enacted         pass\n",
       "1      run      drive   running      pass     adopted          act\n",
       "2   passed  touchdown   wrecked  crossing    approved     declared\n",
       "3    reach    passing       ran      runs     changed      granted\n",
       "4    drive       ball   heading     drive  introduced          ran\n",
       "5  running      yards      runs   highway      passes      adopted\n",
       "6       go     passed    passes   traffic    returned     approved\n",
       "7     look       goal  carrying     route     dropped  legislature\n",
       "8  passing    running    moving     creek       voted       issued\n",
       "9     turn       runs   driving      onto     carried         bill"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [col for col in df.columns if col[1] == \"verbs\"]\n",
    "df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>adjs</th>\n",
       "      <th>adjs</th>\n",
       "      <th>adjs</th>\n",
       "      <th>adjs</th>\n",
       "      <th>adjs</th>\n",
       "      <th>adjs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>happy</th>\n",
       "      <th>happy</th>\n",
       "      <th>negative</th>\n",
       "      <th>negative</th>\n",
       "      <th>visible</th>\n",
       "      <th>visible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pleased</td>\n",
       "      <td>anyone</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>noticeable</td>\n",
       "      <td>surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>surprised</td>\n",
       "      <td>'ll</td>\n",
       "      <td>critical</td>\n",
       "      <td>reaction</td>\n",
       "      <td>evident</td>\n",
       "      <td>shape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worried</td>\n",
       "      <td>everyone</td>\n",
       "      <td>adverse</td>\n",
       "      <td>critical</td>\n",
       "      <td>accessible</td>\n",
       "      <td>relatively</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>glad</td>\n",
       "      <td>'d</td>\n",
       "      <td>favorable</td>\n",
       "      <td>effects</td>\n",
       "      <td>recognizable</td>\n",
       "      <td>inside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sorry</td>\n",
       "      <td>let</td>\n",
       "      <td>significant</td>\n",
       "      <td>serious</td>\n",
       "      <td>available</td>\n",
       "      <td>color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>afraid</td>\n",
       "      <td>ask</td>\n",
       "      <td>serious</td>\n",
       "      <td>behavior</td>\n",
       "      <td>noteworthy</td>\n",
       "      <td>objects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>proud</td>\n",
       "      <td>feel</td>\n",
       "      <td>strong</td>\n",
       "      <td>certain</td>\n",
       "      <td>notable</td>\n",
       "      <td>walls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>satisfied</td>\n",
       "      <td>wants</td>\n",
       "      <td>useful</td>\n",
       "      <td>impact</td>\n",
       "      <td>susceptible</td>\n",
       "      <td>yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>willing</td>\n",
       "      <td>hope</td>\n",
       "      <td>specific</td>\n",
       "      <td>potential</td>\n",
       "      <td>significant</td>\n",
       "      <td>structures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sure</td>\n",
       "      <td>saying</td>\n",
       "      <td>personal</td>\n",
       "      <td>effect</td>\n",
       "      <td>active</td>\n",
       "      <td>skin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           1         6            1          6             1           6\n",
       "        adjs      adjs         adjs       adjs          adjs        adjs\n",
       "       happy     happy     negative   negative       visible     visible\n",
       "0    pleased    anyone     positive   positive    noticeable     surface\n",
       "1  surprised       'll     critical   reaction       evident       shape\n",
       "2    worried  everyone      adverse   critical    accessible  relatively\n",
       "3       glad        'd    favorable    effects  recognizable      inside\n",
       "4      sorry       let  significant    serious     available       color\n",
       "5     afraid       ask      serious   behavior    noteworthy     objects\n",
       "6      proud      feel       strong    certain       notable       walls\n",
       "7  satisfied     wants       useful     impact   susceptible      yellow\n",
       "8    willing      hope     specific  potential   significant  structures\n",
       "9       sure    saying     personal     effect        active        skin"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [col for col in df.columns if col[1] == \"adjs\"]\n",
    "df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>preps</th>\n",
       "      <th>preps</th>\n",
       "      <th>preps</th>\n",
       "      <th>preps</th>\n",
       "      <th>preps</th>\n",
       "      <th>preps</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>from</th>\n",
       "      <th>above</th>\n",
       "      <th>above</th>\n",
       "      <th>after</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>below</td>\n",
       "      <td>page</td>\n",
       "      <td>before</td>\n",
       "      <td>before</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>between</td>\n",
       "      <td>between</td>\n",
       "      <td>here</td>\n",
       "      <td>should</td>\n",
       "      <td>when</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>through</td>\n",
       "      <td>,</td>\n",
       "      <td>debate</td>\n",
       "      <td>discussion</td>\n",
       "      <td>while</td>\n",
       "      <td>during</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>until</td>\n",
       "      <td>until</td>\n",
       "      <td>following</td>\n",
       "      <td>talk</td>\n",
       "      <td>during</td>\n",
       "      <td>later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>since</td>\n",
       "      <td>at</td>\n",
       "      <td>list</td>\n",
       "      <td>below</td>\n",
       "      <td>following</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>into</td>\n",
       "      <td>south</td>\n",
       "      <td>review</td>\n",
       "      <td>link</td>\n",
       "      <td>since</td>\n",
       "      <td>when</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>by</td>\n",
       "      <td>moved</td>\n",
       "      <td>however</td>\n",
       "      <td>article</td>\n",
       "      <td>by</td>\n",
       "      <td>until</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>,</td>\n",
       "      <td>)</td>\n",
       "      <td>talk</td>\n",
       "      <td>here</td>\n",
       "      <td>until</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>after</td>\n",
       "      <td>graduated</td>\n",
       "      <td>link</td>\n",
       "      <td>do</td>\n",
       "      <td>into</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.</td>\n",
       "      <td>into</td>\n",
       "      <td>same</td>\n",
       "      <td>review</td>\n",
       "      <td>but</td>\n",
       "      <td>following</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         1          6          1           6          1          6\n",
       "     preps      preps      preps       preps      preps      preps\n",
       "      from       from      above       above      after      after\n",
       "0       in         in      below        page     before     before\n",
       "1  between    between       here      should       when         he\n",
       "2  through          ,     debate  discussion      while     during\n",
       "3    until      until  following        talk     during      later\n",
       "4    since         at       list       below  following        his\n",
       "5     into      south     review        link      since       when\n",
       "6       by      moved    however     article         by      until\n",
       "7        ,          )       talk        here      until        had\n",
       "8    after  graduated       link          do       into      first\n",
       "9        .       into       same      review        but  following"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [col for col in df.columns if col[1] == \"preps\"]\n",
    "df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1\n",
    "\n",
    "For nouns, the nearest neighbors predominantly belong to the noun category. However, for other categories like verbs, adjectives, and prepositions, there's a higher variation in the types of nearest neighbors, especially as the window size changes. A smaller window size captures more syntactic relationships, leading to a more homogeneous list of nearest neighbors in terms of part-of-speech. In contrast, a larger window size tends to emphasize semantic relationships, resulting in a diverse set of nearest neighbors.\n",
    "\n",
    "- **Nouns:** \n",
    "    - For example, the nearest neighbors for \"dog\" are mostly nouns. In the case of a smaller window size, it's observed that it returns specific animals like cat, rabbit, and chicken. On the other hand, a larger window size captures a broader semantic context, thus returning words such as man, boy, wild, and breed. This suggests that the larger window size perceives the concept of \"dog\" more as an animal in a broader sense.\n",
    "    \n",
    "- **Verbs:** \n",
    "    - \"Pass\" and its variations produce a mix of verbs and non-verbs. For the window size of 1, the results are mostly verbs, while for window size 6, there's a blend.\n",
    "    \n",
    "- **Adjectives:** \n",
    "    - For example, \"happy\" predominantly generates adjectives for the window size of 1. However, with window size 6, the nearest neighbors include verbs and pronouns, indicating a shift towards capturing semantic relationships.\n",
    "    \n",
    "- **Prepositions:** \n",
    "    - For example, the nearest neighbors for \"from\" are mostly prepositions, but for a window size of 6, there are some outliers like \"graduated\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_category = {\"polysemy\": [\"bank\", \"head\", \"fall\", \"book\", \"light\"],\n",
    "                  \"homonymy\": ['lead', 'bear', 'wound', \"apple\", \"well\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15225/15225 [00:00<00:00, 16423.74it/s]\n",
      "100%|██████████| 15225/15225 [00:01<00:00, 15084.30it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 67527.09it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 50212.92it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 46705.33it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 60351.27it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 59319.45it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 43735.28it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 53413.35it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 59290.65it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 50990.72it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 48173.99it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 43974.65it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 45936.18it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 43629.50it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 40866.79it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 41709.36it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 49800.30it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 46812.05it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 52753.81it/s]\n"
     ]
    }
   ],
   "source": [
    "df2 = queries_n_nearest(query_category, vectors_pmi_w, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>polysemy</th>\n",
       "      <th>polysemy</th>\n",
       "      <th>polysemy</th>\n",
       "      <th>polysemy</th>\n",
       "      <th>polysemy</th>\n",
       "      <th>polysemy</th>\n",
       "      <th>polysemy</th>\n",
       "      <th>polysemy</th>\n",
       "      <th>polysemy</th>\n",
       "      <th>polysemy</th>\n",
       "      <th>homonymy</th>\n",
       "      <th>homonymy</th>\n",
       "      <th>homonymy</th>\n",
       "      <th>homonymy</th>\n",
       "      <th>homonymy</th>\n",
       "      <th>homonymy</th>\n",
       "      <th>homonymy</th>\n",
       "      <th>homonymy</th>\n",
       "      <th>homonymy</th>\n",
       "      <th>homonymy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>bank</th>\n",
       "      <th>bank</th>\n",
       "      <th>head</th>\n",
       "      <th>head</th>\n",
       "      <th>fall</th>\n",
       "      <th>fall</th>\n",
       "      <th>book</th>\n",
       "      <th>book</th>\n",
       "      <th>light</th>\n",
       "      <th>light</th>\n",
       "      <th>lead</th>\n",
       "      <th>lead</th>\n",
       "      <th>bear</th>\n",
       "      <th>bear</th>\n",
       "      <th>wound</th>\n",
       "      <th>wound</th>\n",
       "      <th>apple</th>\n",
       "      <th>apple</th>\n",
       "      <th>well</th>\n",
       "      <th>well</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>side</td>\n",
       "      <td>capital</td>\n",
       "      <td>director</td>\n",
       "      <td>coach</td>\n",
       "      <td>falling</td>\n",
       "      <td>fell</td>\n",
       "      <td>books</td>\n",
       "      <td>books</td>\n",
       "      <td>heavy</td>\n",
       "      <td>heavy</td>\n",
       "      <td>backing</td>\n",
       "      <td>role</td>\n",
       "      <td>pine</td>\n",
       "      <td>wild</td>\n",
       "      <td>injuries</td>\n",
       "      <td>injuries</td>\n",
       "      <td>pine</td>\n",
       "      <td>microsoft</td>\n",
       "      <td>however</td>\n",
       "      <td>such</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coast</td>\n",
       "      <td>corporation</td>\n",
       "      <td>chief</td>\n",
       "      <td>chief</td>\n",
       "      <td>fell</td>\n",
       "      <td>spring</td>\n",
       "      <td>novel</td>\n",
       "      <td>published</td>\n",
       "      <td>water</td>\n",
       "      <td>surface</td>\n",
       "      <td>leading</td>\n",
       "      <td>character</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>broken</td>\n",
       "      <td>injury</td>\n",
       "      <td>atari</td>\n",
       "      <td>computers</td>\n",
       "      <td>united</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>railway</td>\n",
       "      <td>railway</td>\n",
       "      <td>club</td>\n",
       "      <td>director</td>\n",
       "      <td>go</td>\n",
       "      <td>falls</td>\n",
       "      <td>story</td>\n",
       "      <td>written</td>\n",
       "      <td>dark</td>\n",
       "      <td>dark</td>\n",
       "      <td>guest</td>\n",
       "      <td>featured</td>\n",
       "      <td>goose</td>\n",
       "      <td>golden</td>\n",
       "      <td>wounds</td>\n",
       "      <td>neck</td>\n",
       "      <td>cherry</td>\n",
       "      <td>os</td>\n",
       "      <td>preserved</td>\n",
       "      <td>many</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>park</td>\n",
       "      <td>northern</td>\n",
       "      <td>career</td>\n",
       "      <td>president</td>\n",
       "      <td>falls</td>\n",
       "      <td>summer</td>\n",
       "      <td>album</td>\n",
       "      <td>story</td>\n",
       "      <td>line</td>\n",
       "      <td>color</td>\n",
       "      <td>take</td>\n",
       "      <td>song</td>\n",
       "      <td>oak</td>\n",
       "      <td>bears</td>\n",
       "      <td>injury</td>\n",
       "      <td>arm</td>\n",
       "      <td>christmas</td>\n",
       "      <td>desktop</td>\n",
       "      <td>list</td>\n",
       "      <td>most</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>africa</td>\n",
       "      <td>branch</td>\n",
       "      <td>black</td>\n",
       "      <td>position</td>\n",
       "      <td>come</td>\n",
       "      <td>beginning</td>\n",
       "      <td>song</td>\n",
       "      <td>novel</td>\n",
       "      <td>large</td>\n",
       "      <td>body</td>\n",
       "      <td>main</td>\n",
       "      <td>away</td>\n",
       "      <td>beaver</td>\n",
       "      <td>mountain</td>\n",
       "      <td>washed</td>\n",
       "      <td>injured</td>\n",
       "      <td>olive</td>\n",
       "      <td>mac</td>\n",
       "      <td>discussion</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>banks</td>\n",
       "      <td>southern</td>\n",
       "      <td>former</td>\n",
       "      <td>former</td>\n",
       "      <td>break</td>\n",
       "      <td>finally</td>\n",
       "      <td>film</td>\n",
       "      <td>wrote</td>\n",
       "      <td>regiment</td>\n",
       "      <td>water</td>\n",
       "      <td>featured</td>\n",
       "      <td>stage</td>\n",
       "      <td>deer</td>\n",
       "      <td>wolf</td>\n",
       "      <td>thrown</td>\n",
       "      <td>leg</td>\n",
       "      <td>bear</td>\n",
       "      <td>hardware</td>\n",
       "      <td>debate</td>\n",
       "      <td>some</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>corporation</td>\n",
       "      <td>valley</td>\n",
       "      <td>member</td>\n",
       "      <td>white</td>\n",
       "      <td>move</td>\n",
       "      <td>ended</td>\n",
       "      <td>series</td>\n",
       "      <td>author</td>\n",
       "      <td>fire</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>supporting</td>\n",
       "      <td>love</td>\n",
       "      <td>bird</td>\n",
       "      <td>blue</td>\n",
       "      <td>stretched</td>\n",
       "      <td>wounds</td>\n",
       "      <td>mini</td>\n",
       "      <td>macintosh</td>\n",
       "      <td>such</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>property</td>\n",
       "      <td>lake</td>\n",
       "      <td>assistant</td>\n",
       "      <td>manager</td>\n",
       "      <td>fallen</td>\n",
       "      <td>forced</td>\n",
       "      <td>game</td>\n",
       "      <td>film</td>\n",
       "      <td>power</td>\n",
       "      <td>low</td>\n",
       "      <td>single</td>\n",
       "      <td>guitar</td>\n",
       "      <td>trout</td>\n",
       "      <td>deep</td>\n",
       "      <td>suffer</td>\n",
       "      <td>knee</td>\n",
       "      <td>desktop</td>\n",
       "      <td>software</td>\n",
       "      <td>there</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>railroad</td>\n",
       "      <td>banks</td>\n",
       "      <td>life</td>\n",
       "      <td>assistant</td>\n",
       "      <td>summer</td>\n",
       "      <td>falling</td>\n",
       "      <td>music</td>\n",
       "      <td>song</td>\n",
       "      <td>battalion</td>\n",
       "      <td>blue</td>\n",
       "      <td>title</td>\n",
       "      <td>goal</td>\n",
       "      <td>maple</td>\n",
       "      <td>dragon</td>\n",
       "      <td>kicked</td>\n",
       "      <td>chest</td>\n",
       "      <td>egg</td>\n",
       "      <td>windows</td>\n",
       "      <td>known</td>\n",
       "      <td>more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>province</td>\n",
       "      <td>centre</td>\n",
       "      <td>minister</td>\n",
       "      <td>brother</td>\n",
       "      <td>walk</td>\n",
       "      <td>winter</td>\n",
       "      <td>episode</td>\n",
       "      <td>series</td>\n",
       "      <td>small</td>\n",
       "      <td>type</td>\n",
       "      <td>bass</td>\n",
       "      <td>big</td>\n",
       "      <td>bears</td>\n",
       "      <td>creek</td>\n",
       "      <td>blow</td>\n",
       "      <td>severe</td>\n",
       "      <td>oak</td>\n",
       "      <td>devices</td>\n",
       "      <td>u</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             1            6          1          6        1          6  \\\n",
       "      polysemy     polysemy   polysemy   polysemy polysemy   polysemy   \n",
       "          bank         bank       head       head     fall       fall   \n",
       "0         side      capital   director      coach  falling       fell   \n",
       "1        coast  corporation      chief      chief     fell     spring   \n",
       "2      railway      railway       club   director       go      falls   \n",
       "3         park     northern     career  president    falls     summer   \n",
       "4       africa       branch      black   position     come  beginning   \n",
       "5        banks     southern     former     former    break    finally   \n",
       "6  corporation       valley     member      white     move      ended   \n",
       "7     property         lake  assistant    manager   fallen     forced   \n",
       "8     railroad        banks       life  assistant   summer    falling   \n",
       "9     province       centre   minister    brother     walk     winter   \n",
       "\n",
       "         1          6          1          6           1          6        1  \\\n",
       "  polysemy   polysemy   polysemy   polysemy    homonymy   homonymy homonymy   \n",
       "      book       book      light      light        lead       lead     bear   \n",
       "0    books      books      heavy      heavy     backing       role     pine   \n",
       "1    novel  published      water    surface     leading  character      dog   \n",
       "2    story    written       dark       dark       guest   featured    goose   \n",
       "3    album      story       line      color        take       song      oak   \n",
       "4     song      novel      large       body        main       away   beaver   \n",
       "5     film      wrote   regiment      water    featured      stage     deer   \n",
       "6   series     author       fire  sometimes  supporting       love     bird   \n",
       "7     game       film      power        low      single     guitar    trout   \n",
       "8    music       song  battalion       blue       title       goal    maple   \n",
       "9  episode     series      small       type        bass        big    bears   \n",
       "\n",
       "          6          1         6          1          6           1        6  \n",
       "   homonymy   homonymy  homonymy   homonymy   homonymy    homonymy homonymy  \n",
       "       bear      wound     wound      apple      apple        well     well  \n",
       "0      wild   injuries  injuries       pine  microsoft     however     such  \n",
       "1       dog     broken    injury      atari  computers      united    other  \n",
       "2    golden     wounds      neck     cherry         os   preserved     many  \n",
       "3     bears     injury       arm  christmas    desktop        list     most  \n",
       "4  mountain     washed   injured      olive        mac  discussion      are  \n",
       "5      wolf     thrown       leg       bear   hardware      debate     some  \n",
       "6      blue  stretched    wounds       mini  macintosh        such     like  \n",
       "7      deep     suffer      knee    desktop   software       there     have  \n",
       "8    dragon     kicked     chest        egg    windows       known     more  \n",
       "9     creek       blow    severe        oak    devices           u      all  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_category = {\"nouns\": [\"soviet\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15225/15225 [00:00<00:00, 41861.32it/s]\n",
      "100%|██████████| 15225/15225 [00:00<00:00, 46028.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>nouns</th>\n",
       "      <th>nouns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>soviet</th>\n",
       "      <th>soviet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>german</td>\n",
       "      <td>russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>british</td>\n",
       "      <td>communist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>israeli</td>\n",
       "      <td>union</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>russian</td>\n",
       "      <td>polish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>french</td>\n",
       "      <td>forces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>polish</td>\n",
       "      <td>russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>japanese</td>\n",
       "      <td>troops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>italian</td>\n",
       "      <td>poland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>iraqi</td>\n",
       "      <td>republic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>allied</td>\n",
       "      <td>germany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1          6\n",
       "      nouns      nouns\n",
       "     soviet     soviet\n",
       "0    german    russian\n",
       "1   british  communist\n",
       "2   israeli      union\n",
       "3   russian     polish\n",
       "4    french     forces\n",
       "5    polish     russia\n",
       "6  japanese     troops\n",
       "7   italian     poland\n",
       "8     iraqi   republic\n",
       "9    allied    germany"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_n_nearest(query_category, vectors_pmi_w, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3\n",
    "-  While embeddings capture various senses of polysemous words effectively, they often fall short with homonyms, barring an exception \"apple.\" Homonymous words are inherently tricky. The embeddings' inability to capture all senses might be influenced by the frequency of usage of each sense in the training data. \n",
    "\n",
    "    - For \"apple\",  8/10 nearest neighbors with window size 1 are related to the fruit meaning, while all 10 nearest neighbors with window size 6 are related to the Apple company.\n",
    "\n",
    "    - For \"bear,\" it's surprising that the embeddings didn't capture the meaning of \"endure\" as a verb, as it is used as a verb quite often. \n",
    "\n",
    "\n",
    "- A smaller window size (w=1) hones in on syntactic relationships, focusing on immediate contexts. A larger window size (w=6), on the other hand, broadens its horizon to capture semantic relationships and multiple senses more effectively.\n",
    "    - For example:\n",
    "        - Book: \n",
    "            - Window size = 1: Nearest neighbors include: \"novel,\" \"story,\" \"film,\" \"song,\" \"music,\" and \"game.\" These terms suggest a strong association with different forms of storytelling and entertainment mediums\n",
    "            \n",
    "            - Window size = 6: Nearest neighbors are: \"published,\" \"written,\" \"wrote,\" \"author,\" and \"series.\" This collection showcases a broader context around the book, focusing on aspects of publishing, writing, and a mix of related media.\n",
    "\n",
    "        - Bank:\n",
    "            - Window size = 1: Nearest neighbors include terms closely related to financial institutions, such as \"company,\" \"insurance,\" \"corporation,\" \"railway,\" and \"banking.\"\n",
    "\n",
    "            - Window size = 6: While it still retains some financial connotations like \"banks,\" \"company,\" and \"capital,\" there are other senses like \"river\" and \"west\" that indicate the riverbank meaning.\n",
    "\n",
    "- It is not likely that a query generates exactly the same nearest neighbors with the two window sizes. However, when the word is noun, very specific and concrete, and has only one meaning, the neighbors with two window sizes can be very similar. For example, \"soviet\" generates very similar neighbors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
